# Network-wide Rule Orchestrator

This repository contains the code and instructions to replicate the results of my Master's Dissertation entitled:: ()[]. 

## **Table of Contents**
- [Installation Process](#installation-process)
- [Repository Structure](#repository-structure)
- [Replicating the Experiments](#replicating-the-experiments)
- [Obtaining the Baseline Alerts](#obtaining-the-baseline-alerts)
  <!---[Understading the code](#understading-the-code)-->

## Installation Process

### 1. Install Vagrant and VirtualBox

- [Vagrant](https://developer.hashicorp.com/vagrant/install)

- [VirtualBox](https://www.virtualbox.org/wiki/Downloads)

### 2. Build the P4 virtual machine

Clone the official repository:

```
git clone https://github.com/p4lang/tutorials.git
```

```
cd tutorial/vm-ubuntu-20.04
```
Run the following command:

```
vagrant up
```

### 3. Adjust the P4 VM settings

Turn off the machine to modify the hardware settings. Extend the number of vCPU cores and memory size so it does not crash during the experiments.

The used configuration for the P4 VM was:
- 22 vCPU cores
- 146 GB of memory

Note: We used a DELL server with substantial resources for our experiments. If using a smaller configuration, carefully monitor the egress number of packets during experiments. If the egress count is significantly smaller than the input for any PCAP, consider increasing the memory space or the number of cores.

### 4. Log into the machine and clone this repository

Update the apt related packages and go to the `Documents` folder:
```
sudo apt update && cd Documents
```

Clone this repository and enter it:
```
git clone  https://github.com/HenriqueBBrum/Network-Wide-Rule-Orchestrator.git && cd Network-Wide-Rule-Orchestrator
```

### 5. Download the CICIDS2017 PCAP files and create a shared folder with the guest system

To download the CICIDS2017 PCAP files, 
1. Go to [this link](https://www.unb.ca/cic/datasets/ids-2017.html);
2. Scroll down to the end of the page;
3. Click the `Download this dataset` and fill in the required information;
4. Enter the `CIC-IDS-2017` directory and then the `PCAPs` directory;
5. Finally download the PCAP for each day of the week.

> :warning: They are big PCAPs, with over 8GB of data, so they take a while to download.

Save the PCAP files into your host machine in one folder. For the guest folder, it must be named `CICIDS2017-PCAPS` and placed at the same level as this repo's folder. The image below illustrates this:
<p align="center">
	<img src="./assets/guest_folder_placement.png" alt="drawing" width="800"/>
</p>


For more information on how to create shared folders with VirtualBox, follow these links:
- [How to Create and Access a Shared Folder in VirtualBox](https://www.makeuseof.com/how-to-create-virtualbox-shared-folder-access/)
- [How to create shared folder for virtual machine on VirtualBox](https://pureinfotech.com/create-shared-folder-virtual-machine-virtualbox/)


### 6. Install the dependencies

The last step is to run the following command to automatically install all required dependencies and configure the enviroment:

```
./install_dependencies.sh
```

> :warning: This script takes a long time to finish, be patitent.

The packages and tools installed are the following:
- tcpreplay
- matplotlib and networkx (Python)
- BMv2 (for custom high performance)
- Snort3

After this process ends, the enviroment is configured and ready to run the experiments. 

## Repository Structure

```
├── assets/
├── baseline_alerts/
├── snort/
├── src/
├── testing/
├── utils/
├── .gitignore
├── install_dependencies.sh
├── README.md
```

- **`assets/`**: The images used in this README file;

- **`baseline_alerts/`**: Contains the alerts generated by Snort 3 when using the CICIDS2017 dataset as input and the following rulesets: Snort 3 Community, Snort 2 Emerging Threats, Snort 3 Registered;

- **`snort/`**: Snort folder with the configuration files, rulesets, and the logs output;

- **`src/`**: Data plane folder with the P4 code and topologies definition;
	- **`include/`**: Folder with files defining the P4 headers and the P4 parser;
	- **`topologies/`**: This folder contains the topologies used in the experiments, with the topology definitons and fowarding for the swtiches;
	- **`main.p4`**: Main P4 file, detailing the functioning of the data plane;
	- **`p4_table_entries*.config`**: The table entries to offload;

- **`testing/`**: The testing and plotting files;
	- **`algorithms_evaluation/`**: The testing and plotting files for the final evaluation with different offloading algorithms and topologies;	
	- **`dataplane_parameters_evaluation/`**: The testing and plotting files for the dataplane parameters evaluation where the dataplane algorithms parameters are evaluated;
	- **`experiment_configuration/`**: The configuration files for each topology. These files contanin configurations for the table entries offloading programa and the experiemnts's Mininet nodes;
	

- **`utils/`**: Folder containing Python files that interact with Mininet and the P4Runtime;
	- **`offload_table_entries.py`**: File responsbile for offloading the table entries to the swtiches according to the offloading algorithm;
	- **`run_P4.py`**: Python file responsible for creating the Mininet enviroment and running an experiment;

- **`.gitignore`**: Files for git to ignore;

- **`/install_dependencies.sh`**: Script used in to install the depedencies and to configure the environment;

- **`/README.md`**: README file with this project's documentation.


## Replicating the Experiments

There are two set of experiments to replicate: the [data plane parameters evaluation](#data-plane-parameters-evaluation), and the [network-wide offloading algorithms evaluation](#network-wide-offloading-algorithms-evaluation). The first evaluates different data plane configurations to decide on the best one. The latter evaluates the network-wide offloading algorithms with two topologies, linear and tree, using different memory availability scenarios in the switches.

### Data plane parameters evaluation

The data plane parameters evaluations assesses three parameters:
- The number of packets to clone for each count-min entry, or _N_.
	- The values tested are: 10, 25, 50, 100, 200, 400, 800
- The time threshold to age the count-min entries, or _T_.
	- The values tested are: 10, 25, 50.
- The width of the count-min sketch hash arrays, or _W_.
	- The values tested are: 256, 512, 1024, 4096, 16834.

> The evaluation experiments with all combinations of the three parameteres (i.e., {N(10), T(10), W(256)}, {N(10), T(10), W(512)}, etc.).

Before running the experiment, there is the need to do some adjustments in the code. First, uncomment the following line in the `utils/offload_table_entries.py` file:

```
print("Installed P4 Program using SetForwardingPipelineConfig on switch "+switch_id)
switch.SetForwardingPipelineConfig(p4info=p4info_helper.p4info, bmv2_json_file_path=bmv2_json)
```
In the data plane parameters evaluation, the topology used does not have forwarding rules; it only clones suspicious packets and sends the packets to a default port. Because of this, these lines must be uncommented to install the P4 program in the data plane.

Second, uncomment the following line in the `src/main.p4` to foward packets to the default port:

```
standard_metadata.egress_spec = DEFAULT_PORT;
```

The last change is to comment (//) the following line in the `src/main.p4` since there are no IPv4 fowarding rules in the siwtche of this experriment:
 ```
ipv4_lpm.apply();
```

With these changes done, create an output folder in any location you like; for example:
```
mkdir ~/Documents/dataplane_parameters_evaluation
```

Then, enter the `dataplane_parameters_evaluation/` folder in the `testing/` directory, and run the following commands in the terminal:
```
cd Network-Wide-Rule-Orchestrator/testing/dataplane_parameters_evaluation
./evaluate_parameters ~/Documents/dataplane_parameters_evaluation
```

> Always provide the full path, rather than a relative path, for the output folder parameter.

The complete parameters evaluation takes a LONG time. There are roughly 55 million packets being sent at a rate of 1000 packest/s (due to BMv2 limited performance), which traslates to 15 hours to evaluate each parameter combination. There are 105 combinations, so it takes a VERY LONG TIME. If you wish to reduce the number of parameters to evaluate, feel free to do so.

After finishing the experiments, it is time to plot them. For this, install Jupyter:
```
pip install jupyterlab
```

With Jupyter installed, go to the `testing/` folder, and run the following command:
```
jupyter lab --NotebookApp.iopub_data_rate_limit=1.0e10
```

Open the `parameters_eval_plots.ipynb` in your browser. Before executing the notebook's cells, update the experiment's input folder and output folder in the `Parse the parameters evaluation experiments` code.
> Change the output folder to avoid collisions with the graphs of this repository.

With the folders updated, strat the kernel and run all cells to plot the graphs. That`s it, the dataplane parameters evaluation is over. Analyze the results and compare it with the ones in the dissertation.


### Network-wide offloading algorithms evaluation

The evaluation of the network-wide offloading algorithms assesses the three table entries offloading algorithms presented in my dissertation in two network topologies with 4 memory availability scenarios:

- The table entries offloading algorithm are:
	- Simple, First-Fit, Best-Fit.
- The topologies evaluated include: 
	- 5 switches linear and 5 switches tree.
 - Memory avialabiltiy scenarios for each switch in the network:
	 - 100%,75%,50%,25%
 
> The memory avialability refers to the amount of table entries a swtich can receive. For example, `100%` means all switches in the network can receive all table entries, whereas `25%` means the switches can receive only 25% of the table entries to offload.

Before conducting the experiments, it is necessary to adjust some parts of the code. If you haven't run the `Data plane parameters evaluation` you can skip this step. Otherwise, go back to the [data plane parameters evaluation](#data-plane-parameters-evaluation), check the modifications done, and perform the opposite action in the code. This means that if the action was to comment, you need to uncomment the mentioned code and vice-versa.

The dataplane parameters are set to: _N_ = 200, _T_ = 10, and _W_ = 16384, just like in the papers experiments.


To start the evaluation, create an output folder in any location you like; for example:
```
mkdir ~/Documents/algorithms_evaluation
```

Then, enter the `algorithms_evaluation/` folder in the `testing` directory, and run the following command in the terminal:
```
./evaluate_algorithms. ~/Documents/algorithms_evaluation
```

> Always provide the full path, rather than a relative path, for the output folder parameter.

This evaluation takes a long time, but not as long as the dataplane parameters evaluation. 

The last step is to generate the CSV table with all results and plot the graphs. For this, install Jupyter if you haven't:

```
pip install jupyterlab
```

Then, in the testing folder, run the following command:
```
jupyter lab --NotebookApp.iopub_data_rate_limit=1.0e10
```

Open the `algorithm_evaluation_plots.ipynb` in your browser. Before executing the notebook's cells, update the experiment's input folder and the output folder in the `Parse the parameters evaluation experiments` section.
> Change the output folder to avoid collisions with the graphs of this repository.

With the folders updated, start the kernel and run all cells to generate the CSV table and plot the graphs. That`s it, the network-wide table entries offloading algorithm evaluation is over. Analyze the results and compare with the ones in my dissertation.

## Obtaining the Baseline Alerts

The baseline alerts used to compare against the experiments alerts were obtained through the following method:

```
snort -c <config-path> --rule-path <rule-path> -r <pcap-file-location> -A alert_json --lua "alert_json = {file = true}"
```

This command runs Snort 3 using the ruleset located in `<rule-path>` and analyzes the network traffic of the PCAP file located at `<pcap-file-location>`. The rulesets used for testing are stored in the `snort/rules` folder. The output alerts file, containing all generated alerts, is saved in the same location where the command is executed.



